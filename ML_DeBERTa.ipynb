{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 11:35:01.779698: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mast\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1591\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1605\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1606\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1607\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1608\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/__init__.py:62\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     ArgumentHandler,\n\u001b[1;32m     51\u001b[0m     CsvPipelineDataFormat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     infer_framework_load_model,\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdepth_estimation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DepthEstimationPipeline\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_question_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentQuestionAnsweringPipeline\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureExtractionPipeline\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfill_mask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FillMaskPipeline\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/document_question_answering.py:29\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     ExplicitEnum,\n\u001b[1;32m     22\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     logging,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkPipeline, build_pipeline_init_args\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquestion_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select_starts_ends\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/question_answering.py:9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SquadExample, SquadFeatures, squad_convert_examples_to_features\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelcard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCard\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizer\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/data/__init__.py:27\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_collator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     DataCollatorForLanguageModeling,\n\u001b[1;32m     17\u001b[0m     DataCollatorForPermutationLanguageModeling,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     default_data_collator,\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glue_compute_metrics, xnli_compute_metrics\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     DataProcessor,\n\u001b[1;32m     30\u001b[0m     InputExample,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     xnli_tasks_num_labels,\n\u001b[1;32m     45\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sklearn_available():\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pearsonr, spearmanr\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score, matthews_corrcoef\n\u001b[1;32m     23\u001b[0m DEPRECATION_WARNING \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Evaluate \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrary. You can have a look at this example script for pointers: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimple_accuracy\u001b[39m(preds, labels):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/__init__.py:84\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     80\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     81\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     86\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    130\u001b[0m     ]\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_module\u001b[39m(module):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_show_versions.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m threadpool_info\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_openmp_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _openmp_parallelism_enabled\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_sys_info\u001b[39m():\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"System information\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import DebertaV2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"filtered_data.csv\", index_col=0)  # 459.728 entries\n",
    "unique_ads = data.drop_duplicates(\n",
    "    subset=[\"ad_creative_body\"]\n",
    ")  # # print(unique_txt.size) 58.449 unique ad text bites to proces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking the length of tokens from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for DebertaV2\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "\n",
    "max_length = 512  # max length of text/tokes for DeBERTa\n",
    "\n",
    "above_limit = 0\n",
    "below_limit = 0\n",
    "more_than_double = 0\n",
    "double_above_limit = 0\n",
    "triple_above_limit = 0\n",
    "\n",
    "for text in unique_ads[\"ad_creative_body\"]:\n",
    "    tokens = tokenizer.encode(\n",
    "        text, add_special_tokens=True\n",
    "    )  # Tokenize the ad text using DebertaV2\n",
    "    token_length = len(tokens)\n",
    "\n",
    "    if token_length > max_length:\n",
    "        above_limit += 1\n",
    "        if token_length >= 1536 and token_length < 2047:\n",
    "            triple_above_limit += 1\n",
    "        if token_length >= 1024 and token_length <= 1536:\n",
    "            double_above_limit += 1\n",
    "        if token_length <= 1024:\n",
    "            more_than_double +=1\n",
    "    else:\n",
    "        below_limit += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of ads within the limit: 58381\n",
      "Amount of ads above the limit: 68\n",
      "Ads above the limit but below 1024 tokens: 60\n",
      "Ads above 1024 tokens but below 1536 tokens: 5\n",
      "Ads above 1536 tokens but below 2047 tokens: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Amount of ads within the limit: {below_limit}\")\n",
    "print(f\"Amount of ads above the limit: {above_limit}\")\n",
    "print(f\"Ads above the limit but below 1024 tokens: {more_than_double}\") # split with 2\n",
    "print(f\"Ads above 1024 tokens but below 1536 tokens: {double_above_limit}\")  # split with 3\n",
    "print(f\"Ads above 1536 tokens but below 2047 tokens: {triple_above_limit}\")  # split with 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the ads through DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"MoritzLaurer/deberta-v3-large-zeroshot-v1.1-all-33\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_template = \"This ad is about {}\"\n",
    "classes_verbalized = [\n",
    "    \"Economy\",\n",
    "    \"Civil Rights\",\n",
    "    \"Healthcare\",\n",
    "    \"Agriculture\",\n",
    "    \"Labor and Employment\",\n",
    "    \"Education and Culture\",\n",
    "    \"Climate\",\n",
    "    \"Immigration\",\n",
    "    \"Transport\",\n",
    "    \"Law and Crime\",\n",
    "    \"Social Welfare\",\n",
    "    \"Housing\",\n",
    "    \"Defense\",\n",
    "    \"Foreign Affair\",\n",
    "    \"Call for Action\",\n",
    "    \"Other\"   #placeholder category\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cr/2b4bjd0d6cscnr8h_66vmrqm0000gn/T/ipykernel_968/3584049382.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_ads[\"num_tokens\"] = num_tokens_list\n"
     ]
    }
   ],
   "source": [
    "# list to store the number of tokens for each ad\n",
    "num_tokens_list = []\n",
    "\n",
    "# Tokenize each ad_creative_body and get the number of tokens\n",
    "for text in unique_ads[\"ad_creative_body\"]:\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)  # Tokenize the text\n",
    "    num_tokens_list.append(len(tokens)) \n",
    "\n",
    "unique_ads[\"num_tokens\"] = num_tokens_list # make new column with the token count for each ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into two df based on limitation\n",
    "below_limit = unique_ads[unique_ads[\"num_tokens\"] <= 512]\n",
    "above_limit = unique_ads[unique_ads[\"num_tokens\"] > 512]\n",
    "\n",
    "# below_limit.to_csv(\"under_512.csv\")\n",
    "#above_limit.to_csv(\"aboev_512.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing text below the limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process ads within limit\n",
    "text_to_vector_below = {}\n",
    "for text in below_limit[\"ad_creative_body\"]:\n",
    "    output = zeroshot_classifier(\n",
    "        text,\n",
    "        classes_verbalized,\n",
    "        hypothesis_template=hypothesis_template,\n",
    "        multi_label=False,\n",
    "    )\n",
    "    text_to_vector_below[text] = {\n",
    "        \"labels\": output[\"labels\"],\n",
    "        \"scores\": output[\"scores\"],\n",
    "    }\n",
    "\n",
    "# Making two new columns\n",
    "below_limit[\"labels\"] = below_limit[\"ad_creative_body\"].map(\n",
    "    lambda text: text_to_vector_below[text][\"labels\"]\n",
    ")\n",
    "below_limit[\"scores\"] = below_limit[\"ad_creative_body\"].map(\n",
    "    lambda text: text_to_vector_below[text][\"scores\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing text above limit (complicated one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_vector_above = (\n",
    "    {}\n",
    ")  # dict to store labels + scores for each ad abover # 512 tokens\n",
    "\n",
    "for text in above_limit[\"ad_creative_body\"]:\n",
    "\n",
    "    total_scores = []  # list to keep scres for each split\n",
    "    all_labels = None  # will store labels\n",
    "\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "    total_splits = (  # figure out how many splits in terms of max l\n",
    "        len(tokens) + max_length - 1\n",
    "    ) // max_length  # // rounding to a single int value\n",
    "\n",
    "    split_size = (  # figure out how to do an equal split on the total tokens (as equal as possible)\n",
    "        len(tokens) + total_splits - 1\n",
    "    ) // total_splits  # // rounding to a single int value\n",
    "\n",
    "    for i in range(0, len(tokens), split_size):\n",
    "        chunk = tokens[i : i + split_size]\n",
    "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "\n",
    "        output = zeroshot_classifier(\n",
    "            chunk_text,\n",
    "            classes_verbalized,\n",
    "            hypothesis_template=hypothesis_template,\n",
    "            multi_label=False,\n",
    "        )\n",
    "\n",
    "        # gather the scores\n",
    "        if all_labels is None:\n",
    "            all_labels = output[\"labels\"]  # store labels on first text bit\n",
    "        total_scores.append(output[\"scores\"])  # Collect all scores for avg\n",
    "\n",
    "    # Average the scores across all chunks\n",
    "    avg_scores = [sum(s) / len(total_scores) for s in zip(*total_scores)]\n",
    "\n",
    "    text_to_vector_above[text] = {\n",
    "        \"labels\": all_labels,  # Use the labels from run throough of the text bit\n",
    "        \"scores\": avg_scores,\n",
    "    }\n",
    "\n",
    "# 2 new columns\n",
    "data[\"labels\"] = data[\"ad_creative_body\"].map(\n",
    "    lambda text: text_to_vector_above[text][\"labels\"]\n",
    ")\n",
    "data[\"scores\"] = data[\"ad_creative_body\"].map(\n",
    "    lambda text: text_to_vector_above[text][\"scores\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## store results from both dataframes back to org DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ads[\"labels\"] = unique_ads[\"ad_creative_body\"].map(\n",
    "    lambda text: text_to_vector_below.get(text, text_to_vector_above.get(text, {})).get(\n",
    "        \"labels\", []\n",
    "    )\n",
    ")\n",
    "unique_ads[\"scores\"] = unique_ads[\"ad_creative_body\"].map(\n",
    "    lambda text: text_to_vector_above.get(text, text_to_vector_above.get(text, {})).get(\n",
    "        \"scores\", []\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic stats on labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort away duplicate ad texts coming from the same politician\n",
    "#label_data_unique = label_data.drop_duplicates(subset=['ad_creative_body', 'page_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 58574 entries, 0 to 58573\n",
      "Data columns (total 21 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Unnamed: 0.2              58574 non-null  int64  \n",
      " 1   Unnamed: 0.1              58574 non-null  int64  \n",
      " 2   Unnamed: 0                58574 non-null  int64  \n",
      " 3   ad_creation_time          58574 non-null  object \n",
      " 4   ad_creative_body          58574 non-null  object \n",
      " 5   spend                     58574 non-null  float64\n",
      " 6   impressions               58574 non-null  float64\n",
      " 7   delivery_by_region        58574 non-null  object \n",
      " 8   demographic_distribution  58574 non-null  object \n",
      " 9   page_id                   58574 non-null  int64  \n",
      " 10  page_name                 58574 non-null  object \n",
      " 11  bylines                   58339 non-null  object \n",
      " 12  id                        58574 non-null  int64  \n",
      " 13  spend_lo                  58574 non-null  int64  \n",
      " 14  spend_hi                  58574 non-null  int64  \n",
      " 15  impressions_lo            58574 non-null  int64  \n",
      " 16  impressions_hi            58574 non-null  int64  \n",
      " 17  party                     58574 non-null  object \n",
      " 18  state                     58574 non-null  object \n",
      " 19  labels                    58574 non-null  object \n",
      " 20  scores                    58574 non-null  object \n",
      "dtypes: float64(2), int64(9), object(10)\n",
      "memory usage: 9.8+ MB\n"
     ]
    }
   ],
   "source": [
    "#label_data_unique.info() # from 459.728 ads to 58.574 ads !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts 'labels' and 'scores' column from object to a list to be able to retrieve the first instance -> top score/top_label\n",
    "\n",
    "label_data_unique[\"labels\"] = label_data_unique[\"labels\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "label_data_unique[\"scores\"] = label_data_unique[\"scores\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Retrieving first instance in each column\n",
    "label_data_unique[\"top_label\"] = label_data_unique[\"labels\"].str[0]\n",
    "label_data_unique[\"top_score\"] = label_data_unique[\"scores\"].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving another CSV file with the two new columns\n",
    "label_data_unique.to_csv(\"deberta_top_labels.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
