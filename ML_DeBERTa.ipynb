{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofiepalmuskronborg/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "#from transformers import pipeline\n",
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from transformers import DebertaV2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"filtered_data.csv\", index_col=0)  # 459.728 entries\n",
    "unique_ads = data.drop_duplicates(\n",
    "    subset=[\"ad_creative_body\"]\n",
    ")  # # print(unique_txt.size) 58.449 unique ad text bites to proces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking the length of tokens from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for DebertaV2\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "\n",
    "max_length = 512  # max length of text/tokes for DeBERTa\n",
    "\n",
    "above_limit = 0\n",
    "below_limit = 0\n",
    "more_than_double = 0\n",
    "double_above_limit = 0\n",
    "triple_above_limit = 0\n",
    "\n",
    "for text in unique_ads[\"ad_creative_body\"]:\n",
    "    tokens = tokenizer.encode(\n",
    "        text, add_special_tokens=True\n",
    "    )  # Tokenize the ad text using DebertaV2\n",
    "    token_length = len(tokens)\n",
    "\n",
    "    if token_length > max_length:\n",
    "        above_limit += 1\n",
    "        if token_length >= 1536 and token_length < 2047:\n",
    "            triple_above_limit += 1\n",
    "        if token_length >= 1024 and token_length <= 1536:\n",
    "            double_above_limit += 1\n",
    "        if token_length <= 1024:\n",
    "            more_than_double +=1\n",
    "    else:\n",
    "        below_limit += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of ads within the limit: 58381\n",
      "Amount of ads above the limit: 68\n",
      "Ads above the limit but below 1024 tokens: 60\n",
      "Ads above 1024 tokens but below 1536 tokens: 5\n",
      "Ads above 1536 tokens but below 2047 tokens: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Amount of ads within the limit: {below_limit}\")\n",
    "print(f\"Amount of ads above the limit: {above_limit}\")\n",
    "print(f\"Ads above the limit but below 1024 tokens: {more_than_double}\") # split with 2\n",
    "print(f\"Ads above 1024 tokens but below 1536 tokens: {double_above_limit}\")  # split with 3\n",
    "print(f\"Ads above 1536 tokens but below 2047 tokens: {triple_above_limit}\")  # split with 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the ads through DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"MoritzLaurer/deberta-v3-large-zeroshot-v1.1-all-33\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_template = \"This ad is about {}\"\n",
    "classes_verbalized = [\n",
    "    \"Economy\",\n",
    "    \"Civil Rights\",\n",
    "    \"Healthcare\",\n",
    "    \"Agriculture\",\n",
    "    \"Labor and Employment\",\n",
    "    \"Education and Culture\",\n",
    "    \"Climate\",\n",
    "    \"Immigration\",\n",
    "    \"Transport\",\n",
    "    \"Law and Crime\",\n",
    "    \"Social Welfare\",\n",
    "    \"Housing\",\n",
    "    \"Defense\",\n",
    "    \"Foreign Affair\",\n",
    "    \"Call for Action\",\n",
    "    \"Other\"   #placeholder category\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cr/2b4bjd0d6cscnr8h_66vmrqm0000gn/T/ipykernel_968/3584049382.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_ads[\"num_tokens\"] = num_tokens_list\n"
     ]
    }
   ],
   "source": [
    "# list to store the number of tokens for each ad\n",
    "num_tokens_list = []\n",
    "\n",
    "# Tokenize each ad_creative_body and get the number of tokens\n",
    "for text in unique_ads[\"ad_creative_body\"]:\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)  # Tokenize the text\n",
    "    num_tokens_list.append(len(tokens)) \n",
    "\n",
    "unique_ads[\"num_tokens\"] = num_tokens_list # make new column with the token count for each ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into two df based on limitation\n",
    "below_limit = unique_ads[unique_ads[\"num_tokens\"] <= 512]\n",
    "above_limit = unique_ads[unique_ads[\"num_tokens\"] > 512]\n",
    "\n",
    "# below_limit.to_csv(\"under_512.csv\")\n",
    "#above_limit.to_csv(\"aboev_512.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing text below the limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process ads within limit\n",
    "text_to_vector_below = {}\n",
    "for text in below_limit[\"ad_creative_body\"]:\n",
    "    output = zeroshot_classifier(\n",
    "        text,\n",
    "        classes_verbalized,\n",
    "        hypothesis_template=hypothesis_template,\n",
    "        multi_label=False,\n",
    "    )\n",
    "    text_to_vector_below[text] = {\n",
    "        \"labels\": output[\"labels\"],\n",
    "        \"scores\": output[\"scores\"],\n",
    "    }\n",
    "\n",
    "# Making two new columns\n",
    "below_limit[\"labels\"] = below_limit[\"ad_creative_body\"].map(\n",
    "    lambda text: text_to_vector_below[text][\"labels\"]\n",
    ")\n",
    "below_limit[\"scores\"] = below_limit[\"ad_creative_body\"].map(\n",
    "    lambda text: text_to_vector_below[text][\"scores\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing text above limit (complicated one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_vector_above = (\n",
    "    {}\n",
    ")  # dict to store labels + scores for each ad abover # 512 tokens\n",
    "\n",
    "for text in above_limit[\"ad_creative_body\"]:\n",
    "\n",
    "    total_scores = []  # list to keep scres for each split\n",
    "    all_labels = None  # will store labels\n",
    "\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "    total_splits = (  # figure out how many splits in terms of max l\n",
    "        len(tokens) + max_length - 1\n",
    "    ) // max_length  # // rounding to a single int value\n",
    "\n",
    "    split_size = (  # figure out how to do an equal split on the total tokens (as equal as possible)\n",
    "        len(tokens) + total_splits - 1\n",
    "    ) // total_splits  # // rounding to a single int value\n",
    "\n",
    "    for i in range(0, len(tokens), split_size):\n",
    "        chunk = tokens[i : i + split_size]\n",
    "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "\n",
    "        output = zeroshot_classifier(\n",
    "            chunk_text,\n",
    "            classes_verbalized,\n",
    "            hypothesis_template=hypothesis_template,\n",
    "            multi_label=False,\n",
    "        )\n",
    "\n",
    "        # gather the scores\n",
    "        if all_labels is None:\n",
    "            all_labels = output[\"labels\"]  # store labels on first text bit\n",
    "        total_scores.append(output[\"scores\"])  # Collect all scores for avg\n",
    "\n",
    "    # Average the scores across all chunks\n",
    "    avg_scores = [sum(s) / len(total_scores) for s in zip(*total_scores)]\n",
    "\n",
    "    text_to_vector_above[text] = {\n",
    "        \"labels\": all_labels,  # Use the labels from run throough of the text bit\n",
    "        \"scores\": avg_scores,\n",
    "    }\n",
    "\n",
    "# 2 new columns\n",
    "above_limit[\"labels\"] = above_limit[\"ad_creative_body\"].map(\n",
    "    lambda text: text_to_vector_above[text][\"labels\"]\n",
    ")\n",
    "above_limit[\"scores\"] = above_limit[\"ad_creative_body\"].map(\n",
    "    lambda text: text_to_vector_above[text][\"scores\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## store results from both dataframes back to org DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting dataframes together so all data is gathere\n",
    "vertical_concat = pd.concat([below_limit, above_limit], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the additional rows back into origianl df (filtered_data) based on ad text\n",
    "additional_rows = vertical_concat[\n",
    "    [\"ad_creative_body\", \"labels\", \"scores\", \"top_label\", \"top_score\"]\n",
    "]\n",
    "\n",
    "søndag = pd.merge(data, additional_rows, on=\"ad_creative_body\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "søndag.to_csv(\"søndagsmagi.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the numbers er ok\n",
    "# sort away duplicate ad texts coming from the same politician -> 58.574\n",
    "label_data_unique = søndag.drop_duplicates(subset=['ad_creative_body', 'page_id'])\n",
    "\n",
    "# sort away duplicate text in general -> 58.449 rows ^ 125 duplicate texts when comparing\n",
    "label_data_unique = søndag.drop_duplicates(['ad_creative_body']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts 'labels' and 'scores' column from object to a list to be able to retrieve the first instance -> top score/top_label\n",
    "\n",
    "label_data_unique[\"labels\"] = label_data_unique[\"labels\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "label_data_unique[\"scores\"] = label_data_unique[\"scores\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Retrieving first instance in each column\n",
    "label_data_unique[\"top_label\"] = label_data_unique[\"labels\"].str[0]\n",
    "label_data_unique[\"top_score\"] = label_data_unique[\"scores\"].str[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
